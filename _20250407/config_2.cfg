# # # # # # # # # # # # config.cfg # # # # # # # # # # # #
[paths]
train = null
dev = null
vectors = null
init_tok2vec = null

[system]
gpu_allocator = "pytorch"
seed = 0

[nlp]
lang = "pl"
pipeline = ["transformer", "ner"]
batch_size = 128

[nlp.tokenizer]
@tokenizers = "spacy.Tokenizer.v1"

[components]

[components.ner]
factory = "ner"
incorrect_spans_key = null
moves = null
scorer = {"@scorers":"spacy.ner_scorer.v1"}
prune_heads = false

[components.ner.model]
@architectures = "spacy.TransitionBasedParser.v2"
state_type = "ner"
extra_state_tokens = false
hidden_width = 64
maxout_pieces = 2
use_upper = true
n_tok2vec = 0

[components.ner.model.tok2vec]
@architectures = "spacy-transformers.TransformerListener.v1"
grad_factor = 1.0
pooling = {"@layers":"reduce_mean.v1"}
get_spans = {"@span_getters":"spacy-transformers.get_doc_spans.v1"}

[components.transformer]
factory = "transformer"

[components.transformer.model]
@architectures = "spacy-transformers.TransformerModel.v3"
name = "allegro/herbert-base-cased"

[corpora]
[corpora.dev]
@readers = "spacy.Corpus.v1"
path = ${paths.dev}

[corpora.train]
@readers = "spacy.Corpus.v1"
path = ${paths.train}

[training]
train_corpus = "corpora.train"
dev_corpus = "corpora.dev"
seed = ${system.seed}
gpu_allocator = ${system.gpu_allocator}
frozen_components = []

[training.optimizer]
@optimizers = "Adam.v1"

[initialize]
vectors = ${paths.vectors}
# # # # # # # # # # # # KONIEC PLIKU # # # # # # # # # # # #